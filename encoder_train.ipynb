{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def loadbar(iteration, total, prefix=\"\", suffix=\"\", decimal=0, \n",
    "            length=100, fill=\"=\", extras=\"\"):\n",
    "    per_val = iteration*100/float(total)\n",
    "    \n",
    "    percent = (\"{0:.\" + str(decimal) + \"f}\").format(per_val)   \n",
    "    cur_percent = ( ' ' * (3-len(str(round(per_val)))) + percent)\n",
    "    \n",
    "    filledLen = int(length * iteration//total)\n",
    "    if per_val == 100:\n",
    "        bar = fill * filledLen + \".\" * (length - filledLen)\n",
    "    else:\n",
    "        bar = fill * filledLen + \">\" + \".\" * (length - filledLen - 1)\n",
    "        \n",
    "    print(f\"\\r{prefix} [{bar}] {cur_percent}% {suffix}\", end=\"\\r\")\n",
    "    if iteration == total: \n",
    "        print(f\"\\r{prefix} [{bar}] {cur_percent}% {suffix} {extras}\", end=\"\\n\")\n",
    "        \n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effb64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc09f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Const\n",
    "PAR_N_FRAMES = 160\n",
    "EM_SIZE = 256\n",
    "MEL_N_CHANNELS = 40\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOSS_DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e46648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals Vars\n",
    "nspeaker = 8\n",
    "nutter = 10\n",
    "\n",
    "nworker = 0\n",
    "# nworker = os.cpu_count()\n",
    "\n",
    "hlsize = 256\n",
    "nlayer = 3\n",
    "learnrate =1e-4\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "limt = 30\n",
    "limv = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647b3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "class RandomCycler:  \n",
    "    def __init__(self, source):\n",
    "        self.all_items = list(source)\n",
    "        self.next_items = []\n",
    "    \n",
    "    def sample(self, count: int):\n",
    "        shuffle = lambda l: random.sample(l, len(l))\n",
    "        \n",
    "        out = []\n",
    "        while count > 0:\n",
    "            if count >= len(self.all_items):\n",
    "                out.extend(shuffle(list(self.all_items)))\n",
    "                count -= len(self.all_items)\n",
    "                continue\n",
    "            n = min(count, len(self.next_items))\n",
    "            out.extend(self.next_items[:n])\n",
    "            count -= n\n",
    "            self.next_items = self.next_items[n:]\n",
    "            if len(self.next_items) == 0:\n",
    "                self.next_items = shuffle(list(self.all_items))\n",
    "        return out\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.sample(1)[0]\n",
    "\n",
    "class Utterance:\n",
    "    def __init__(self, uid, frames):\n",
    "        self.id = uid\n",
    "        self.frames = frames\n",
    "\n",
    "    def random_partial(self, n_frames=PAR_N_FRAMES):\n",
    "        if self.frames.shape[0] == n_frames:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = np.random.randint(0, self.frames.shape[0] - n_frames)\n",
    "        end = start + n_frames\n",
    "        return self.frames[start:end], (start, end)   \n",
    "    \n",
    "class Speaker:\n",
    "    def __init__(self, sid, data):\n",
    "        self.id = sid\n",
    "        self.data = data\n",
    "        self.utterances = None\n",
    "        self.utterance_cycler = None\n",
    "        \n",
    "    def _load_utterances(self):\n",
    "        self.utterances = [Utterance(idx, arr) for idx, arr in self.data]\n",
    "        self.utterance_cycler = RandomCycler(self.utterances)\n",
    "               \n",
    "    def random_partial(self, count, n_frames=PAR_N_FRAMES):\n",
    "        if self.utterances is None: self._load_utterances()\n",
    "        utterances = self.utterance_cycler.sample(count)\n",
    "        return [(u,) + u.random_partial(n_frames) for u in utterances]\n",
    "\n",
    "class SpeakerBatch:\n",
    "    def __init__(self, speakers, utterances_per_speaker, n_frames=PAR_N_FRAMES):\n",
    "        self.partials = {s: s.random_partial(utterances_per_speaker, n_frames) for s in speakers}\n",
    "        self.data = np.array([frames for s in speakers for _, frames, _ in self.partials[s]])   \n",
    "        \n",
    "class SpeakerVerificationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe      \n",
    "        self.speakers = [Speaker(x, self.df[self.df.speaker_id==x][[\"file_id\", \"frames\"]].to_numpy()) for x in self.df.speaker_id.unique()]\n",
    "        self.speaker_cycler = RandomCycler(self.speakers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(1e10)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return next(self.speaker_cycler)\n",
    "    \n",
    "class SpeakerVerificationDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, speakers_per_batch, utterances_per_speaker, sampler=None, \n",
    "                 batch_sampler=None, num_workers=0, pin_memory=False, timeout=0, \n",
    "                 worker_init_fn=None):\n",
    "        self.utterances_per_speaker = utterances_per_speaker\n",
    "\n",
    "        super().__init__(\n",
    "            dataset=dataset, \n",
    "            batch_size=speakers_per_batch, \n",
    "            shuffle=False, \n",
    "            sampler=sampler, \n",
    "            batch_sampler=batch_sampler, \n",
    "            num_workers=num_workers,\n",
    "            collate_fn=self.collate, \n",
    "            pin_memory=pin_memory, \n",
    "            drop_last=False, \n",
    "            timeout=timeout, \n",
    "            worker_init_fn=worker_init_fn\n",
    "        )\n",
    "\n",
    "    def collate(self, speakers):\n",
    "        return SpeakerBatch(speakers, self.utterances_per_speaker, n_frames=PAR_N_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf5c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SpeakerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        learning_rate,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.loss_device = LOSS_DEVICE\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=MEL_N_CHANNELS,\n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True).to(DEVICE)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, \n",
    "                                out_features=EM_SIZE).to(DEVICE)\n",
    "        self.relu = torch.nn.ReLU().to(DEVICE)\n",
    "        \n",
    "        self.similarity_weight = nn.Parameter(torch.tensor([10.])).to(self.loss_device)\n",
    "        self.similarity_bias = nn.Parameter(torch.tensor([-5.])).to(self.loss_device)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(self.loss_device)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def do_gradient_ops(self):\n",
    "        self.similarity_weight.grad *= 0.01\n",
    "        self.similarity_bias.grad *= 0.01\n",
    "            \n",
    "        clip_grad_norm_(self.parameters(), 3, norm_type=2)\n",
    "    \n",
    "    def forward(self, utterances, hidden_init=None):\n",
    "        out, (hidden, cell) = self.lstm(utterances, hidden_init)\n",
    "        \n",
    "        embeds_raw = self.relu(self.linear(hidden[-1]))\n",
    "        \n",
    "        return embeds_raw / (torch.norm(embeds_raw, dim=1, keepdim=True) + 1e-5)        \n",
    "    \n",
    "    def similarity_matrix(self, embeds):\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "        \n",
    "        centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "        centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "        centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "        centroids_excl /= (utterances_per_speaker - 1)\n",
    "        centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "        sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                                 speakers_per_batch).to(self.loss_device)\n",
    "        mask_matrix = 1 - np.eye(speakers_per_batch, dtype=int)\n",
    "        for j in range(speakers_per_batch):\n",
    "            mask = np.where(mask_matrix[j])[0]\n",
    "            sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n",
    "            sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n",
    "        \n",
    "        return sim_matrix * self.similarity_weight + self.similarity_bias\n",
    "    \n",
    "    def loss(self, embeds):\n",
    "        speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "        \n",
    "        sim_matrix = self.similarity_matrix(embeds)\n",
    "        sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker, \n",
    "                                         speakers_per_batch))\n",
    "        ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "        target = torch.from_numpy(ground_truth).long().to(self.loss_device)\n",
    "        loss = self.loss_fn(sim_matrix, target)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=int)[0]\n",
    "            labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "            preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n",
    "            eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "            \n",
    "        return loss, eer\n",
    "    \n",
    "def sync(device: torch.device):\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5775dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>frames</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5536-43363-0006</td>\n",
       "      <td>5536</td>\n",
       "      <td>[[5.1189186e-06, 8.7852095e-06, 1.9026568e-06,...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5536-43363-0010</td>\n",
       "      <td>5536</td>\n",
       "      <td>[[2.8548186e-06, 2.4248714e-06, 3.9052784e-06,...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5536-43363-0007</td>\n",
       "      <td>5536</td>\n",
       "      <td>[[1.979813e-05, 1.698165e-05, 6.14011e-06, 1.5...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5536-43363-0014</td>\n",
       "      <td>5536</td>\n",
       "      <td>[[1.6595812e-05, 7.0819824e-06, 5.835367e-06, ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5536-43363-0001</td>\n",
       "      <td>5536</td>\n",
       "      <td>[[8.575194e-06, 3.4151853e-06, 1.0975212e-06, ...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file_id speaker_id  \\\n",
       "0  5536-43363-0006       5536   \n",
       "1  5536-43363-0010       5536   \n",
       "2  5536-43363-0007       5536   \n",
       "3  5536-43363-0014       5536   \n",
       "4  5536-43363-0001       5536   \n",
       "\n",
       "                                              frames  file_count  \n",
       "0  [[5.1189186e-06, 8.7852095e-06, 1.9026568e-06,...          59  \n",
       "1  [[2.8548186e-06, 2.4248714e-06, 3.9052784e-06,...          59  \n",
       "2  [[1.979813e-05, 1.698165e-05, 6.14011e-06, 1.5...          59  \n",
       "3  [[1.6595812e-05, 7.0819824e-06, 5.835367e-06, ...          59  \n",
       "4  [[8.575194e-06, 3.4151853e-06, 1.0975212e-06, ...          59  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(np.load(\"data/encoder_librispeech_valid.npz\", allow_pickle=True)[\"data\"], columns=[\"file_id\", \"speaker_id\", \"frames\"])\n",
    "\n",
    "df = df.merge(df.groupby(\"speaker_id\")[\"file_id\"].count().reset_index().rename(columns={\"file_id\": \"file_count\"}), on=\"speaker_id\")\n",
    "\n",
    "df = df[df[\"file_count\"]>=nutter]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c610afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2144, 4), (536, 4))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cbc4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = SpeakerVerificationDataLoader(SpeakerVerificationDataset(X_train), nspeaker, nutter, num_workers=nworker)\n",
    "valid_dl = SpeakerVerificationDataLoader(SpeakerVerificationDataset(X_valid), nspeaker, nutter, num_workers=nworker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74d130a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] 30/30 [==================================================] 100% - loss: 2.0720 - eer: 0.4768 \n",
      "Epoch [2/5] 30/30 [==================================================] 100% - loss: 2.0037 - eer: 0.4875 \n",
      "Epoch [3/5] 30/30 [==================================================] 100% - loss: 1.7409 - eer: 0.3125 \n",
      "Epoch [4/5] 30/30 [==================================================] 100% - loss: 1.3673 - eer: 0.2482 \n",
      "Epoch [5/5] 30/30 [==================================================] 100% - loss: 1.4552 - eer: 0.2821 \n"
     ]
    }
   ],
   "source": [
    "class EncoderTrainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def common_step(self, batch_idx, batch, stage=None):\n",
    "        inputs = torch.from_numpy(batch.data).to(DEVICE)\n",
    "        sync(DEVICE)\n",
    "\n",
    "        embeds = self.model(inputs)\n",
    "        sync(DEVICE)\n",
    "\n",
    "        embeds_loss = embeds.view((nspeaker, nutter, -1)).to(LOSS_DEVICE)\n",
    "        return self.model.loss(embeds_loss)\n",
    "    \n",
    "    def training_step(self, batch_idx, batch):\n",
    "        loss, eer = self.common_step(batch_idx, batch, \"train\")\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.model.do_gradient_ops()\n",
    "        self.model.optimizer.step()\n",
    "        return loss, eer\n",
    "    \n",
    "    def validation_step(self, batch_idx, batch):\n",
    "        return self.common_step(batch_idx, batch, \"val\")\n",
    "    \n",
    "    def validation_epoch(self, valid_dl):\n",
    "        res = {\n",
    "            \"loss\": 0,\n",
    "            \"eer\": 0\n",
    "        }\n",
    "        bsize = len(valid_dl) if limv == -1 else limv\n",
    "        for idx, batch in enumerate(valid_dl):\n",
    "            if idx > bsize-1: break\n",
    "            loss, eer = self.validation_step(idx, batch)\n",
    "            res[\"loss\"] += loss\n",
    "            res[\"eer\"] += eer\n",
    "        return res[\"loss\"]/bsize, res[\"eer\"]/bsize\n",
    "    \n",
    "    def fit(self, epochs, train_dl, valid_dl=None):\n",
    "        res = {\n",
    "            \"loss\": 0,\n",
    "            \"eer\": 0\n",
    "        }\n",
    "        \n",
    "        bsize = len(train_dl) if limt == -1 else limt\n",
    "        for epoch in range(epochs):\n",
    "            i = 0\n",
    "            \n",
    "            cur_epoch = ( '0' * (len(str(epochs))-len(str(epoch+1))) + str(epoch+1))\n",
    "            cur_batch = ( '0' * (len(str(bsize))-len(str(i+1))) + str(i+1))\n",
    "            \n",
    "            loadbar(i, bsize, f\"Epoch [{cur_epoch}/{epochs}] {cur_batch}/{bsize}\", length=50)             \n",
    "            for idx, batch in enumerate(train_dl):\n",
    "                \n",
    "                if idx > bsize-1: break\n",
    "                \n",
    "                loss, eer = self.training_step(idx, batch)\n",
    "                s = f\"- loss: {loss:0.4f} - eer: {eer:0.4f}\"\n",
    "                \n",
    "                res[\"loss\"] += loss\n",
    "                res[\"eer\"] += eer\n",
    "                \n",
    "                cur_batch = ( '0' * (len(str(bsize))-len(str(i+1))) + str(i+1))\n",
    "                p = f\"Epoch [{cur_epoch}/{epochs}] {cur_batch}/{bsize}\"\n",
    "                \n",
    "                if (i+1 == bsize) and (valid_dl!=None):\n",
    "                    s = f\"- loss: {res['loss']/bsize:0.4f} - eer: {res['loss']/bsize:0.4f}\"\n",
    "                    \n",
    "                    loss, eer = self.validation_epoch(valid_dl)\n",
    "                    e =f\" - val_loss: {loss:0.4f} - val_eer: {eer:0.4f}\"\n",
    "                    \n",
    "                    loadbar(i+1, bsize, p, s, extras=e, length=50)\n",
    "                else:\n",
    "                    loadbar(i+1, bsize, p, s, length=50)\n",
    "                    \n",
    "                i+=1\n",
    "                \n",
    "#                 break\n",
    "#             break\n",
    "\n",
    "model = SpeakerEncoder(hlsize, nlayer, learnrate)\n",
    "trainer = EncoderTrainer(model)\n",
    "\n",
    "trainer.fit(epochs, train_dl)\n",
    "\n",
    "# trainer.fit(epochs, train_dl, valid_dl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0c9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e0d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73848868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a52f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
