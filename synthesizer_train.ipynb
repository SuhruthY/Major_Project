{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1787a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def loadbar(iteration, total, prefix=\"\", suffix=\"\", decimal=0, \n",
    "            length=100, fill=\"=\", extras=\"\"):\n",
    "    per_val = iteration*100/float(total)\n",
    "    \n",
    "    percent = (\"{0:.\" + str(decimal) + \"f}\").format(per_val)   \n",
    "    cur_percent = ( ' ' * (3-len(str(round(per_val)))) + percent)\n",
    "    \n",
    "    filledLen = int(length * iteration//total)\n",
    "    if per_val == 100:\n",
    "        bar = fill * filledLen + \".\" * (length - filledLen)\n",
    "    else:\n",
    "        bar = fill * filledLen + \">\" + \".\" * (length - filledLen - 1)\n",
    "        \n",
    "    print(f\"\\r{prefix} [{bar}] {cur_percent}% {suffix}\", end=\"\\r\")\n",
    "    if iteration == total: \n",
    "        print(f\"\\r{prefix} [{bar}] {cur_percent}% {suffix} {extras}\", end=\"\\n\")\n",
    "        \n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed77cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_output_ref = None\n",
    "_replicas_ref = None\n",
    "\n",
    "def data_parallel_workaround(model, *input):\n",
    "    global _output_ref\n",
    "    global _replicas_ref\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    output_device = device_ids[0]\n",
    "    replicas = torch.nn.parallel.replicate(model, device_ids)\n",
    "    # input.shape = (num_args, batch, ...)\n",
    "    inputs = torch.nn.parallel.scatter(input, device_ids)\n",
    "    # inputs.shape = (num_gpus, num_args, batch/num_gpus, ...)\n",
    "    replicas = replicas[:len(inputs)]\n",
    "    outputs = torch.nn.parallel.parallel_apply(replicas, inputs)\n",
    "    y_hat = torch.nn.parallel.gather(outputs, output_device)\n",
    "    _output_ref = outputs\n",
    "    _replicas_ref = replicas\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "class ValueWindow():\n",
    "  def __init__(self, window_size=100):\n",
    "    self._window_size = window_size\n",
    "    self._values = []\n",
    "\n",
    "  def append(self, x):\n",
    "    self._values = self._values[-(self._window_size - 1):] + [x]\n",
    "\n",
    "  @property\n",
    "  def sum(self):\n",
    "    return sum(self._values)\n",
    "\n",
    "  @property\n",
    "  def count(self):\n",
    "    return len(self._values)\n",
    "\n",
    "  @property\n",
    "  def average(self):\n",
    "    return self.sum / max(1, self.count)\n",
    "\n",
    "  def reset(self):\n",
    "    self._values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print = lambda val, total, substitute=0: f'{substitute}' * (len(str(total))-len(str(val))) + str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5b3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from unidecode import unidecode\n",
    "\n",
    "import inflect\n",
    "inflect = inflect.engine()\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35f3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "limt = 10\n",
    "limv = 10\n",
    "\n",
    "lime = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6547c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hparams\n",
    "class HParams:\n",
    "    num_mels = 80\n",
    "    max_abs_value = 4.   \n",
    "    symmetric_mels = True                                           \n",
    "    speaker_embedding_size = 256\n",
    "\n",
    "    tts_embed_dims = 512                     \n",
    "    tts_encoder_dims = 256\n",
    "    tts_decoder_dims = 128\n",
    "    tts_postnet_dims = 512\n",
    "    tts_encoder_K = 5\n",
    "    tts_lstm_dims = 1024\n",
    "    tts_postnet_K = 5\n",
    "    tts_num_highways = 4\n",
    "    tts_dropout = 0.5\n",
    "    tts_stop_threshold = -3.4   \n",
    "    tts_clip_grad_norm = 1.0                                          \n",
    "\n",
    "    tts_schedule = [(2,  1e-3,  20_000,  12),  \n",
    "                    (2,  5e-4,  40_000,  12),   \n",
    "                    (2,  2e-4,  80_000,  12),   \n",
    "                    (2,  1e-4, 160_000,  12),   \n",
    "                    (2,  3e-5, 320_000,  12),  \n",
    "                    (2,  1e-5, 640_000,  12)]  \n",
    "\n",
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb612498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to Sequence\n",
    "symbols = list(\"_~ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\'\\\"(),-.:;? \")\n",
    "symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "\n",
    "abbreviations = [(re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "    (\"mrs\", \"misess\"),\n",
    "    (\"mr\", \"mister\"),\n",
    "    (\"dr\", \"doctor\"),\n",
    "    (\"st\", \"saint\"),\n",
    "    (\"co\", \"company\"),\n",
    "    (\"jr\", \"junior\"),\n",
    "    (\"maj\", \"major\"),\n",
    "    (\"gen\", \"general\"),\n",
    "    (\"drs\", \"doctors\"),\n",
    "    (\"rev\", \"reverend\"),\n",
    "    (\"lt\", \"lieutenant\"),\n",
    "    (\"hon\", \"honorable\"),\n",
    "    (\"sgt\", \"sergeant\"),\n",
    "    (\"capt\", \"captain\"),\n",
    "    (\"esq\", \"esquire\"),\n",
    "    (\"ltd\", \"limited\"),\n",
    "    (\"col\", \"colonel\"),\n",
    "    (\"ft\", \"fort\"),\n",
    "]]\n",
    "\n",
    "def expand_dollars(m):\n",
    "    parts = m.group(1).split(\".\")\n",
    "\n",
    "    dollars = int(parts[0]) if parts[0] else 0\n",
    "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0 \n",
    "\n",
    "    if dollars and cents:\n",
    "        return \"%s %s, %s %s\" % (dollars, \"dollars\", cents, \"cents\")\n",
    "    elif dollars:\n",
    "        return \"%s %s\" % (dollars, \"dollars\")\n",
    "    elif cents:\n",
    "        return \"%s %s\" % (cents, \"cents\")\n",
    "    else:\n",
    "        return \"zero dollars\"\n",
    "\n",
    "def expand_number(m):\n",
    "    num = int(m.group(0))\n",
    "    if num > 1000 and num < 3000:\n",
    "        if num == 2000:\n",
    "            return \"two thousand\"\n",
    "        elif num > 2000 and num < 2010:\n",
    "            return \"two thousand \" + inflect.number_to_words(num % 100)\n",
    "        elif num % 100 == 0:\n",
    "            return inflect.number_to_words(num // 100) + \" hundred\"\n",
    "        else:\n",
    "            return inflect.number_to_words(num, andword=\"\", zero=\"oh\", group=2).replace(\", \", \" \")\n",
    "    else:\n",
    "        return inflect.number_to_words(num, andword=\"\")\n",
    "    \n",
    "def normalize_numbers(text):\n",
    "    text = re.sub(re.compile(r\"([0-9][0-9\\,]+[0-9])\"), lambda m:m.group(1).replace(\",\", \"\"), text)\n",
    "    text = re.sub(re.compile(r\"Â£([0-9\\,]*[0-9]+)\"), r\"\\1 pounds\", text)\n",
    "    text = re.sub(re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\"), expand_dollars, text)\n",
    "    text = re.sub(re.compile(r\"([0-9]+\\.[0-9]+)\"), lambda m:m.group(1).replace(\".\", \" point \"), text)\n",
    "    text = re.sub(re.compile(r\"[0-9]+(st|nd|rd|th)\"), lambda m:inflect.number_to_words(m.group(0)), text)\n",
    "    text = re.sub(re.compile(r\"[0-9]+\"), expand_number, text)\n",
    "    return text\n",
    "\n",
    "# normalize_numbers(\"I bought a $923.43 necklace at the Black Market for our 25th Anniversary\")\n",
    "\n",
    "def clean(text):\n",
    "    text = unidecode(text)\n",
    "    text = text.lower()\n",
    "    text = normalize_numbers(text)\n",
    "    for regex, replacement in abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    text = re.sub(re.compile(r\"\\s+\"), \" \", text)\n",
    "    return text\n",
    "\n",
    "def text2seq(text):   \n",
    "    text = clean(text)\n",
    "    seq = [symbol_to_id[s] for s in text if s in symbol_to_id and s not in (\"_\", \"~\")]\n",
    "    seq.append(symbol_to_id[\"~\"])\n",
    "    return seq \n",
    "\n",
    "# text2seq(\"Sword Art Online\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c3bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "class SynthesizerDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        text, mel, embed, idx = self.data[index]       \n",
    "        text = np.asarray(text2seq(text))\n",
    "        return text.astype(np.int32), mel.T.astype(np.float32), embed.astype(np.float32), idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate_synthesizer(batch, r):\n",
    "    x_lens = [len(x[0]) for x in batch]\n",
    "    max_x_len = max(x_lens)\n",
    "\n",
    "    chars = [pad1d(x[0], max_x_len) for x in batch]\n",
    "    chars = np.stack(chars)\n",
    "\n",
    "    spec_lens = [x[1].shape[-1] for x in batch]\n",
    "    max_spec_len = max(spec_lens) + 1 \n",
    "    if max_spec_len % r != 0:\n",
    "        max_spec_len += r - max_spec_len % r \n",
    "\n",
    "    if hparams.symmetric_mels:\n",
    "        mel_pad_value = -1 * hparams.max_abs_value\n",
    "    else:\n",
    "        mel_pad_value = 0\n",
    "\n",
    "    mel = [pad2d(x[1], max_spec_len, pad_value=mel_pad_value) for x in batch]\n",
    "    mel = np.stack(mel)\n",
    "\n",
    "    embeds = np.array([x[2] for x in batch])\n",
    "\n",
    "    indices = [x[3] for x in batch]\n",
    "\n",
    "    chars = torch.tensor(chars).long()\n",
    "    mel = torch.tensor(mel)\n",
    "    embeds = torch.tensor(embeds)\n",
    "\n",
    "    return chars, mel, embeds, indices\n",
    "\n",
    "def pad1d(x, max_len, pad_value=0):\n",
    "    return np.pad(x, (0, max_len - len(x)), mode=\"constant\", constant_values=pad_value)\n",
    "\n",
    "def pad2d(x, max_len, pad_value=0):\n",
    "    return np.pad(x, ((0, 0), (0, max_len - x.shape[-1])), mode=\"constant\", constant_values=pad_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8021381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tacotron 2\n",
    "class HighwayNetwork(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(size, size)\n",
    "        self.W2 = nn.Linear(size, size)\n",
    "        self.W1.bias.data.fill_(0.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.W1(x)\n",
    "        x2 = self.W2(x)\n",
    "        g = torch.sigmoid(x2)\n",
    "        y = g * F.relu(x1) + (1. - g) * x\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dims, num_chars, encoder_dims, K, num_highways, dropout):\n",
    "        super().__init__()\n",
    "        prenet_dims = (encoder_dims, encoder_dims)\n",
    "        cbhg_channels = encoder_dims\n",
    "        self.embedding = nn.Embedding(num_chars, embed_dims)\n",
    "        self.pre_net = PreNet(embed_dims, fc1_dims=prenet_dims[0], fc2_dims=prenet_dims[1],\n",
    "                              dropout=dropout)\n",
    "        self.cbhg = CBHG(K=K, in_channels=cbhg_channels, channels=cbhg_channels,\n",
    "                         proj_channels=[cbhg_channels, cbhg_channels],\n",
    "                         num_highways=num_highways)\n",
    "\n",
    "    def forward(self, x, speaker_embedding=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pre_net(x)\n",
    "        x.transpose_(1, 2)\n",
    "        x = self.cbhg(x)\n",
    "        if speaker_embedding is not None:\n",
    "            x = self.add_speaker_embedding(x, speaker_embedding)\n",
    "        return x\n",
    "\n",
    "    def add_speaker_embedding(self, x, speaker_embedding):\n",
    "        batch_size = x.size()[0]\n",
    "        num_chars = x.size()[1]\n",
    "\n",
    "        if speaker_embedding.dim() == 1:\n",
    "            idx = 0\n",
    "        else:\n",
    "            idx = 1\n",
    "\n",
    "        speaker_embedding_size = speaker_embedding.size()[idx]\n",
    "        \n",
    "        e = speaker_embedding.repeat_interleave(num_chars, dim=idx)\n",
    "        e = e.reshape(batch_size, speaker_embedding_size, num_chars)\n",
    "        e = e.transpose(1, 2)\n",
    "        \n",
    "        x = torch.cat((x, e), 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BatchNormConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, relu=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel, stride=1, padding=kernel // 2, bias=False)\n",
    "        self.bnorm = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x) if self.relu is True else x\n",
    "        return self.bnorm(x)\n",
    "\n",
    "\n",
    "class CBHG(nn.Module):\n",
    "    def __init__(self, K, in_channels, channels, proj_channels, num_highways):\n",
    "        super().__init__()\n",
    "\n",
    "        self._to_flatten = []\n",
    "\n",
    "        self.bank_kernels = [i for i in range(1, K + 1)]\n",
    "        self.conv1d_bank = nn.ModuleList()\n",
    "        for k in self.bank_kernels:\n",
    "            conv = BatchNormConv(in_channels, channels, k)\n",
    "            self.conv1d_bank.append(conv)\n",
    "\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n",
    "\n",
    "        self.conv_project1 = BatchNormConv(len(self.bank_kernels) * channels, proj_channels[0], 3)\n",
    "        self.conv_project2 = BatchNormConv(proj_channels[0], proj_channels[1], 3, relu=False)\n",
    "\n",
    "        if proj_channels[-1] != channels:\n",
    "            self.highway_mismatch = True\n",
    "            self.pre_highway = nn.Linear(proj_channels[-1], channels, bias=False)\n",
    "        else:\n",
    "            self.highway_mismatch = False\n",
    "\n",
    "        self.highways = nn.ModuleList()\n",
    "        for i in range(num_highways):\n",
    "            hn = HighwayNetwork(channels)\n",
    "            self.highways.append(hn)\n",
    "\n",
    "        self.rnn = nn.GRU(channels, channels // 2, batch_first=True, bidirectional=True)\n",
    "        self._to_flatten.append(self.rnn)\n",
    "\n",
    "        self._flatten_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._flatten_parameters()\n",
    "\n",
    "        residual = x\n",
    "        seq_len = x.size(-1)\n",
    "        conv_bank = []\n",
    "\n",
    "        for conv in self.conv1d_bank:\n",
    "            c = conv(x)\n",
    "            conv_bank.append(c[:, :, :seq_len])\n",
    "\n",
    "        conv_bank = torch.cat(conv_bank, dim=1)\n",
    "\n",
    "        x = self.maxpool(conv_bank)[:, :, :seq_len]\n",
    "\n",
    "        x = self.conv_project1(x)\n",
    "        x = self.conv_project2(x)\n",
    "\n",
    "        x = x + residual\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.highway_mismatch is True:\n",
    "            x = self.pre_highway(x)\n",
    "        for h in self.highways: x = h(x)\n",
    "\n",
    "        x, _ = self.rnn(x)\n",
    "        return x\n",
    "\n",
    "    def _flatten_parameters(self):\n",
    "        [m.flatten_parameters() for m in self._to_flatten]\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    def __init__(self, in_dims, fc1_dims=256, fc2_dims=128, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.p, training=True)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.p, training=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn_dims):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(attn_dims, attn_dims, bias=False)\n",
    "        self.v = nn.Linear(attn_dims, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_seq_proj, query, t):\n",
    "        query_proj = self.W(query).unsqueeze(1)\n",
    "\n",
    "        u = self.v(torch.tanh(encoder_seq_proj + query_proj))\n",
    "        scores = F.softmax(u, dim=1)\n",
    "\n",
    "        return scores.transpose(1, 2)\n",
    "\n",
    "\n",
    "class LSA(nn.Module):\n",
    "    def __init__(self, attn_dim, kernel_size=31, filters=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, filters, padding=(kernel_size - 1) // 2, kernel_size=kernel_size, bias=True)\n",
    "        self.L = nn.Linear(filters, attn_dim, bias=False)\n",
    "        self.W = nn.Linear(attn_dim, attn_dim, bias=True) \n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        self.cumulative = None\n",
    "        self.attention = None\n",
    "\n",
    "    def init_attention(self, encoder_seq_proj):\n",
    "        device = next(self.parameters()).device  \n",
    "        b, t, c = encoder_seq_proj.size()\n",
    "        self.cumulative = torch.zeros(b, t, device=device)\n",
    "        self.attention = torch.zeros(b, t, device=device)\n",
    "\n",
    "    def forward(self, encoder_seq_proj, query, t, chars):\n",
    "\n",
    "        if t == 0: self.init_attention(encoder_seq_proj)\n",
    "\n",
    "        processed_query = self.W(query).unsqueeze(1)\n",
    "\n",
    "        location = self.cumulative.unsqueeze(1)\n",
    "        processed_loc = self.L(self.conv(location).transpose(1, 2))\n",
    "\n",
    "        u = self.v(torch.tanh(processed_query + encoder_seq_proj + processed_loc))\n",
    "        u = u.squeeze(-1)\n",
    "\n",
    "        u = u * (chars != 0).float()\n",
    "\n",
    "        scores = F.softmax(u, dim=1)\n",
    "        self.attention = scores\n",
    "        self.cumulative = self.cumulative + self.attention\n",
    "\n",
    "        return scores.unsqueeze(-1).transpose(1, 2)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    max_r = 20\n",
    "    def __init__(self, n_mels, encoder_dims, decoder_dims, lstm_dims,\n",
    "                 dropout, speaker_embedding_size):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"r\", torch.tensor(1, dtype=torch.int))\n",
    "        self.n_mels = n_mels\n",
    "        prenet_dims = (decoder_dims * 2, decoder_dims * 2)\n",
    "        self.prenet = PreNet(n_mels, fc1_dims=prenet_dims[0], fc2_dims=prenet_dims[1],\n",
    "                             dropout=dropout)\n",
    "        self.attn_net = LSA(decoder_dims)\n",
    "        self.attn_rnn = nn.GRUCell(encoder_dims + prenet_dims[1] + speaker_embedding_size, decoder_dims)\n",
    "        self.rnn_input = nn.Linear(encoder_dims + decoder_dims + speaker_embedding_size, lstm_dims)\n",
    "        self.res_rnn1 = nn.LSTMCell(lstm_dims, lstm_dims)\n",
    "        self.res_rnn2 = nn.LSTMCell(lstm_dims, lstm_dims)\n",
    "        self.mel_proj = nn.Linear(lstm_dims, n_mels * self.max_r, bias=False)\n",
    "        self.stop_proj = nn.Linear(encoder_dims + speaker_embedding_size + lstm_dims, 1)\n",
    "\n",
    "    def zoneout(self, prev, current, p=0.1):\n",
    "        device = next(self.parameters()).device  \n",
    "        mask = torch.zeros(prev.size(), device=device).bernoulli_(p)\n",
    "        return prev * mask + current * (1 - mask)\n",
    "\n",
    "    def forward(self, encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                hidden_states, cell_states, context_vec, t, chars):\n",
    "\n",
    "        batch_size = encoder_seq.size(0)\n",
    "        \n",
    "        attn_hidden, rnn1_hidden, rnn2_hidden = hidden_states\n",
    "        rnn1_cell, rnn2_cell = cell_states\n",
    "\n",
    "        prenet_out = self.prenet(prenet_in)\n",
    "\n",
    "        attn_rnn_in = torch.cat([context_vec, prenet_out], dim=-1)\n",
    "        attn_hidden = self.attn_rnn(attn_rnn_in.squeeze(1), attn_hidden)\n",
    "\n",
    "        scores = self.attn_net(encoder_seq_proj, attn_hidden, t, chars)\n",
    "\n",
    "        context_vec = scores @ encoder_seq\n",
    "        context_vec = context_vec.squeeze(1)\n",
    "\n",
    "        x = torch.cat([context_vec, attn_hidden], dim=1)\n",
    "        x = self.rnn_input(x)\n",
    "\n",
    "        rnn1_hidden_next, rnn1_cell = self.res_rnn1(x, (rnn1_hidden, rnn1_cell))\n",
    "        if self.training:\n",
    "            rnn1_hidden = self.zoneout(rnn1_hidden, rnn1_hidden_next)\n",
    "        else:\n",
    "            rnn1_hidden = rnn1_hidden_next\n",
    "        x = x + rnn1_hidden\n",
    "\n",
    "        rnn2_hidden_next, rnn2_cell = self.res_rnn2(x, (rnn2_hidden, rnn2_cell))\n",
    "        if self.training:\n",
    "            rnn2_hidden = self.zoneout(rnn2_hidden, rnn2_hidden_next)\n",
    "        else:\n",
    "            rnn2_hidden = rnn2_hidden_next\n",
    "        x = x + rnn2_hidden\n",
    "\n",
    "        mels = self.mel_proj(x)\n",
    "        mels = mels.view(batch_size, self.n_mels, self.max_r)[:, :, :self.r]\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        s = torch.cat((x, context_vec), dim=1)\n",
    "        s = self.stop_proj(s)\n",
    "        stop_tokens = torch.sigmoid(s)\n",
    "\n",
    "        return mels, scores, hidden_states, cell_states, context_vec, stop_tokens\n",
    "\n",
    "\n",
    "class Tacotron(nn.Module):\n",
    "    def __init__(self, embed_dims, num_chars, encoder_dims, decoder_dims, n_mels, \n",
    "                 fft_bins, postnet_dims, encoder_K, lstm_dims, postnet_K, num_highways,\n",
    "                 dropout, stop_threshold, speaker_embedding_size):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.lstm_dims = lstm_dims\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.decoder_dims = decoder_dims\n",
    "        self.speaker_embedding_size = speaker_embedding_size\n",
    "        self.encoder = Encoder(embed_dims, num_chars, encoder_dims,\n",
    "                               encoder_K, num_highways, dropout)\n",
    "        self.encoder_proj = nn.Linear(encoder_dims + speaker_embedding_size, decoder_dims, bias=False)\n",
    "        self.decoder = Decoder(n_mels, encoder_dims, decoder_dims, lstm_dims,\n",
    "                               dropout, speaker_embedding_size)\n",
    "        self.postnet = CBHG(postnet_K, n_mels, postnet_dims,\n",
    "                            [postnet_dims, fft_bins], num_highways)\n",
    "        self.post_proj = nn.Linear(postnet_dims, fft_bins, bias=False)\n",
    "\n",
    "        self.init_model()\n",
    "        self.num_params()\n",
    "\n",
    "        self.register_buffer(\"step\", torch.zeros(1, dtype=torch.long))\n",
    "        self.register_buffer(\"stop_threshold\", torch.tensor(stop_threshold, dtype=torch.float32))\n",
    "\n",
    "    @property\n",
    "    def r(self):\n",
    "        return self.decoder.r.item()\n",
    "\n",
    "    @r.setter\n",
    "    def r(self, value):\n",
    "        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n",
    "\n",
    "    def forward(self, x, m, speaker_embedding):\n",
    "        device = next(self.parameters()).device  # use same device as parameters\n",
    "\n",
    "        self.step += 1\n",
    "        batch_size, _, steps  = m.size()\n",
    "\n",
    "        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n",
    "        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "\n",
    "        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n",
    "\n",
    "        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n",
    "        \n",
    "        encoder_seq = self.encoder(x, speaker_embedding)\n",
    "        encoder_seq_proj = self.encoder_proj(encoder_seq)\n",
    "\n",
    "        mel_outputs, attn_scores, stop_outputs = [], [], []\n",
    "\n",
    "        for t in range(0, steps, self.r):\n",
    "            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n",
    "            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n",
    "                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                             hidden_states, cell_states, context_vec, t, x)\n",
    "            mel_outputs.append(mel_frames)\n",
    "            attn_scores.append(scores)\n",
    "            stop_outputs.extend([stop_tokens] * self.r)\n",
    "\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=2)\n",
    "\n",
    "        postnet_out = self.postnet(mel_outputs)\n",
    "        linear = self.post_proj(postnet_out)\n",
    "        linear = linear.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.cat(attn_scores, 1)\n",
    "        stop_outputs = torch.cat(stop_outputs, 1)\n",
    "\n",
    "        return mel_outputs, linear, attn_scores, stop_outputs\n",
    "\n",
    "    def generate(self, x, speaker_embedding=None, steps=2000):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device  \n",
    "\n",
    "        batch_size, _  = x.size()\n",
    "\n",
    "        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n",
    "        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n",
    "\n",
    "        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n",
    "        cell_states = (rnn1_cell, rnn2_cell)\n",
    "\n",
    "        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n",
    "\n",
    "        context_vec = torch.zeros(batch_size, self.encoder_dims + self.speaker_embedding_size, device=device)\n",
    "\n",
    "        encoder_seq = self.encoder(x, speaker_embedding)\n",
    "        encoder_seq_proj = self.encoder_proj(encoder_seq)\n",
    "\n",
    "        mel_outputs, attn_scores, stop_outputs = [], [], []\n",
    "\n",
    "        for t in range(0, steps, self.r):\n",
    "            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n",
    "            mel_frames, scores, hidden_states, cell_states, context_vec, stop_tokens = \\\n",
    "            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n",
    "                         hidden_states, cell_states, context_vec, t, x)\n",
    "            mel_outputs.append(mel_frames)\n",
    "            attn_scores.append(scores)\n",
    "            stop_outputs.extend([stop_tokens] * self.r)\n",
    "            if (stop_tokens > 0.5).all() and t > 10: break\n",
    "\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=2)\n",
    "\n",
    "        postnet_out = self.postnet(mel_outputs)\n",
    "        linear = self.post_proj(postnet_out)\n",
    "\n",
    "\n",
    "        linear = linear.transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.cat(attn_scores, 1)\n",
    "        stop_outputs = torch.cat(stop_outputs, 1)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        return mel_outputs, linear, attn_scores\n",
    "\n",
    "    def init_model(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1: nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def get_step(self):\n",
    "        return self.step.data.item()\n",
    "\n",
    "    def reset_step(self):\n",
    "        self.step = self.step.data.new_tensor(1)\n",
    "\n",
    "    def log(self, path, msg):\n",
    "        with open(path, \"a\") as f:\n",
    "            print(msg, file=f)\n",
    "\n",
    "    def load(self, path, optimizer=None):\n",
    "        device = next(self.parameters()).device\n",
    "        checkpoint = torch.load(str(path), map_location=device)\n",
    "        self.load_state_dict(checkpoint[\"model_state\"])\n",
    "\n",
    "        if \"optimizer_state\" in checkpoint and optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "\n",
    "    def save(self, path, optimizer=None):\n",
    "        if optimizer is not None:\n",
    "            torch.save({\n",
    "                \"model_state\": self.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "            }, str(path))\n",
    "        else:\n",
    "            torch.save({\n",
    "                \"model_state\": self.state_dict(),\n",
    "            }, str(path))\n",
    "\n",
    "\n",
    "    def num_params(self, print_out=True):\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
    "        if print_out:\n",
    "            print(\"Trainable Parameters: %.3fM\" % parameters)\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436c7ce",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8762b60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1866, 4), (467, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"data/synthesizer_librispeech_valid.npz\", allow_pickle=True)[\"data\"]\n",
    "\n",
    "X_train, X_valid = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba8a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =  SynthesizerDataset(X_train)\n",
    "valid_ds =  SynthesizerDataset(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65bea315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    for session in hparams.tts_schedule:\n",
    "        _, _, _, batch_size = session\n",
    "        if batch_size % torch.cuda.device_count() != 0:\n",
    "            raise ValueError(\"`batch_size` must be evenly divisible by n_gpus!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9baf6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 30.870M\n"
     ]
    }
   ],
   "source": [
    "model = Tacotron(embed_dims=hparams.tts_embed_dims,\n",
    "                     num_chars=len(symbols),\n",
    "                     encoder_dims=hparams.tts_encoder_dims,\n",
    "                     decoder_dims=hparams.tts_decoder_dims,\n",
    "                     n_mels=hparams.num_mels,\n",
    "                     fft_bins=hparams.num_mels,\n",
    "                     postnet_dims=hparams.tts_postnet_dims,\n",
    "                     encoder_K=hparams.tts_encoder_K,\n",
    "                     lstm_dims=hparams.tts_lstm_dims,\n",
    "                     postnet_K=hparams.tts_postnet_K,\n",
    "                     num_highways=hparams.tts_num_highways,\n",
    "                     dropout=hparams.tts_dropout,\n",
    "                     stop_threshold=hparams.tts_stop_threshold,\n",
    "                     speaker_embedding_size=hparams.speaker_embedding_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4885381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/10] 10/10 [==================================================] 100% - loss: 2.5027 \n",
      "Epoch [02/10] 10/10 [==================================================] 100% - loss: 1.5938 \n",
      "Epoch [03/10] 10/10 [==================================================] 100% - loss: 1.6945 \n",
      "Epoch [04/10] 10/10 [==================================================] 100% - loss: 1.4333 \n",
      "Epoch [05/10] 10/10 [==================================================] 100% - loss: 1.3499 \n",
      "Epoch [06/10] 10/10 [==================================================] 100% - loss: 1.4654 \n",
      "Epoch [07/10] 10/10 [==================================================] 100% - loss: 1.0425 \n",
      "Epoch [08/10] 10/10 [==================================================] 100% - loss: 0.7982 \n",
      "Epoch [09/10] 10/10 [==================================================] 100% - loss: 0.9530 \n",
      "Epoch [10/10] 10/10 [==================================================] 100% - loss: 1.0875 \n"
     ]
    }
   ],
   "source": [
    "class SynthesizerTrainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())                  \n",
    "           \n",
    "    def fit(self, train_ds, valid_ds=None):\n",
    "        for i, session in enumerate(hparams.tts_schedule):            \n",
    "            current_step = model.get_step()\n",
    "            r, lr, max_step, batch_size = session\n",
    "            training_steps = max_step - current_step\n",
    "            \n",
    "            if current_step >= max_step:\n",
    "                if i == len(hparams.tts_schedule) - 1:\n",
    "                    model.save(weights_fpath, optimizer)\n",
    "                    break\n",
    "                else: continue\n",
    "\n",
    "            model.r = r\n",
    "\n",
    "            for p in self.optimizer.param_groups: p[\"lr\"] = lr\n",
    "\n",
    "            collate_fn = partial(collate_synthesizer, r=r)\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "            if valid_ds != None: \n",
    "                valid_dl = DataLoader(valid_ds, batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "            \n",
    "            total_iters = len(train_dl)                  \n",
    "            steps_per_epoch = np.ceil(total_iters / batch_size).astype(np.int32)\n",
    "            epochs = np.ceil(training_steps / steps_per_epoch).astype(np.int32) if lime==-1 else lime\n",
    "            \n",
    "            bsize = steps_per_epoch if limt==-1 else limt\n",
    "    \n",
    "            for epoch in range(1, epochs+1):\n",
    "                i = 0\n",
    "                p = f\"Epoch [{pretty_print(epoch, epochs)}/{epochs}] {pretty_print(i, bsize)}/{bsize}\"\n",
    "                loadbar(i, bsize, p, length=50)\n",
    "                for batch_idx, (texts, mels, embeds, idx) in enumerate(train_dl, 1): \n",
    "                    \n",
    "                    if batch_idx > limt: break\n",
    "                    \n",
    "                    step = model.get_step()\n",
    "                        \n",
    "                    stop = torch.ones(mels.shape[0], mels.shape[2])                    \n",
    "                    for j, k in enumerate(idx): stop[j, :int(k)-1] = 0\n",
    "                        \n",
    "                    texts = texts.to(device) \n",
    "                    mels = mels.to(device)\n",
    "                    embeds = embeds.to(device)\n",
    "                    stop = stop.to(device)\n",
    "\n",
    "                    if device.type == \"cuda\" and torch.cuda.device_count() > 1:\n",
    "                        m1_hat, m2_hat, attention, stop_pred = data_parallel_workaround(model, texts, mels, embeds)\n",
    "                    else:\n",
    "                        m1_hat, m2_hat, attention, stop_pred = model(texts, mels, embeds)\n",
    "\n",
    "                    m1_loss = F.mse_loss(m1_hat, mels) + F.l1_loss(m1_hat, mels)\n",
    "                    m2_loss = F.mse_loss(m2_hat, mels)\n",
    "                    stop_loss = F.binary_cross_entropy(stop_pred, stop)\n",
    "\n",
    "                    loss = m1_loss + m2_loss + stop_loss\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()        \n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    p = f\"Epoch [{pretty_print(epoch, epochs)}/{epochs}] {pretty_print(i+1, bsize)}/{bsize}\"\n",
    "                    s = f\"- loss: {loss:.4f}\"\n",
    "                    \n",
    "                    if (i+1 == bsize) and (valid_ds!=None):                      \n",
    "                        loss = self.validation_epoch(valid_dl)\n",
    "                        loadbar(i+1, bsize, p, s, extras=f\"- val_loss: {loss:0.4f}\", length=50)\n",
    "                    else:\n",
    "                        loadbar(i+1, bsize, p, s, length=50)\n",
    "\n",
    "                    if step >= max_step: break\n",
    "                    i += 1        \n",
    "\n",
    "                    if save_every != 0 and step % save_every == 0 :\n",
    "                        model.save(weights_fpath, optimizer)\n",
    "                   \n",
    "                \n",
    "#                     break   \n",
    "#                 break\n",
    "            break\n",
    "            \n",
    "trainer = SynthesizerTrainer(model)\n",
    "\n",
    "# trainer.fit(train_ds)\n",
    "\n",
    "trainer.fit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd6505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc504cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99d1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22798c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
